{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy.stats import boxcox\n","from sklearn.preprocessing import normalize\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","from lightgbm import LGBMRegressor\n","import lightgbm as lgb\n","from sklearn.preprocessing import StandardScaler\n","\n","client=pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\")\n","elec=pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\")\n","fore=pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\")\n","gas=pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\")\n","hist=pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\")\n","train = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/train.csv')\n","location=pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv')\n","location.dropna(inplace=True)\n","location.drop(columns='county_name',inplace=True)\n","def preprocess():\n"," \n","    train['datetime']=pd.to_datetime(train['datetime'])\n","    elec['forecast_date']=pd.to_datetime(elec['forecast_date'])\n","    elec.rename(columns={'forecast_date':'datetime'},inplace=True)\n","\n","    \n","    \n","    fore['forecast_datetime']=pd.to_datetime(fore['forecast_datetime'])\n","    fore.rename(columns={'forecast_datetime':'datetime'},inplace=True)\n","\n","\n","\n","    fore['latitude']=fore['latitude'].astype('float').round(1)\n","    fore['longitude']=fore['longitude'].astype('float').round(1)\n","    fore_location=pd.merge(fore,location,on=['latitude','longitude'],how='left').dropna(axis=0).drop(columns=['hours_ahead','origin_datetime','latitude','longitude','data_block_id'])\n","    fore_location['county']=fore_location['county'].astype('int')\n","    \n","    for cols in fore_location.columns:      \n","        if(cols!='county' and cols!='datetime' and cols!='data_block_id'):\n","            new_name=f'forecast_{cols}'\n","            fore_location.rename(columns={cols:new_name},inplace=True)    \n","    \n","    fore_location['datetime']=pd.to_datetime(fore_location['datetime'])\n","    \n","    fore_location_overall=fore_location.groupby(by=['datetime']).mean().drop(columns=['county']).reset_index()\n","\n","    fore_location_overall['datetime']=pd.to_datetime(fore_location_overall['datetime'])\n","\n","    fore_location_county=fore_location.groupby(by=['datetime','county']).mean().reset_index()\n","\n","    fore_location_county['datetime']=pd.to_datetime(fore_location_overall['datetime'])\n","    \n","    \n","    for cols in fore_location_overall.columns:\n","        if(cols!='datetime' and cols!='data_block_id' and cols!='county'):\n","            new_name=f'overall_{cols}'\n","            fore_location_overall.rename(columns={cols:new_name},inplace=True)\n","    for cols in fore_location_county.columns:\n","        if(cols!='datetime' and cols!='county' and cols!='data_block_id'):\n","            new_name=f'local_{cols}'\n","            fore_location_county.rename(columns={cols:new_name},inplace=True)\n","\n","\n","            \n","    hist['latitude']=hist['latitude'].astype('float').round(1)\n","    hist['longitude']=hist['longitude'].astype('float').round(1)\n","    location_df_history=pd.merge(hist,location,on=['latitude','longitude'],how='left').dropna().drop(columns=['latitude','longitude'])\n","\n","    for cols in location_df_history.columns:\n","        if(cols!='county' and cols!='datetime' and cols!='data_block_id'):\n","            new_name=f'historic_{cols}'\n","            location_df_history.rename(columns={cols:new_name},inplace=True)\n","    location_df_history['county']=location_df_history['county'].astype('int')\n","    location_df_history['datetime']=pd.to_datetime(location_df_history['datetime'])\n","    \n","    df_overall_historic=location_df_history.groupby(by=['datetime']).mean().reset_index().drop(columns=['county','data_block_id'])\n","    df_overall_historic['datetime']=pd.to_datetime(df_overall_historic['datetime'])\n","    df_overall_historic=pd.merge(df_overall_historic,location_df_history[['datetime', 'data_block_id']],on=['datetime'],how='left')\n","    \n","    df_local_historic=location_df_history.groupby(by=['datetime','county']).mean().reset_index().drop(columns=['data_block_id'])\n","    df_local_historic['datetime']=pd.to_datetime(df_local_historic['datetime'])\n","    df_local_historic=pd.merge(df_local_historic,location_df_history[['datetime', 'data_block_id']],on=['datetime'],how='left')\n","    \n","    for cols in df_overall_historic.columns:\n","        if(cols!='datetime' and cols!='data_block_id' and cols!='county'):\n","            new_name=f'overall_{cols}'\n","            df_overall_historic.rename(columns={cols:new_name},inplace=True)\n","    for cols in df_local_historic.columns:\n","        if(cols!='datetime' and cols!='county' and cols!='data_block_id'):\n","            new_name=f'local_{cols}'\n","            df_local_historic.rename(columns={cols:new_name},inplace=True)\n","            \n","            \n","    train['year']=train['datetime'].dt.year\n","    train['month']=train['datetime'].dt.month\n","    train['day']=train['datetime'].dt.day\n","    train['hour']=train['datetime'].dt.hour            \n","    \n","    \n","    elec['hour'] = elec['datetime'].dt.hour\n","    \n","    \n","    \n","#     merged_data= pd.merge(train,client.drop(columns = ['date']), how='left', on=['data_id', 'county', 'is_business', 'product_type'])\n","    merged_data= pd.merge(train,gas[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n","    merged_data= pd.merge(merged_data,client.drop(columns = ['date']), how='left', on=['data_block_id', 'county', 'is_business', 'product_type'])\n","    merged_data= pd.merge(merged_data,elec[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour', 'data_block_id'])\n","    merged_data= pd.merge(merged_data,fore_location_overall, how='left', on=['datetime'])\n","    merged_data= pd.merge(merged_data,fore_location_county, how='left', on=['datetime', 'county'])\n","    \n","    \n","    \n","    \n","    df_overall_historic['hour']= df_overall_historic['datetime'].dt.hour\n","    df_local_historic['hour']= df_local_historic['datetime'].dt.hour\n","\n","    df_overall_historic.drop_duplicates(inplace=True)\n","    df_local_historic.drop_duplicates(inplace=True)\n","    df_overall_historic.drop('datetime', axis= 1, inplace= True)\n","    df_local_historic.drop('datetime', axis= 1, inplace= True)\n","\n","    \n","    merged_data= merged_data.merge(df_overall_historic, how='left', on=['data_block_id', 'hour'])\n","\n","    merged_data= merged_data.merge(df_local_historic, how='left', on=['data_block_id', 'county', 'hour'])\n","   \n","    merged_data=merged_data.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n","\n","    merged_data.drop(['level_0', 'level_1', 'row_id', 'data_block_id'], axis= 1, inplace= True)\n","    return merged_data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["full_merged_df=preprocess()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# full_merged_df.to_csv('enfit.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["full_merged_df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def generate_features(df):\n","    df[\"dayofyear\"] = df[\"datetime\"].dt.dayofyear\n","    df[\"sin(dayofyear)\"] = np.sin(np.pi * df[\"dayofyear\"] / 183)\n","    df[\"cos(dayofyear)\"] = np.cos(np.pi * df[\"dayofyear\"] / 183)\n","    df[\"sin(hour)\"] = np.sin(np.pi * df[\"hour\"] / 12)\n","    df[\"cos(hour)\"] = np.cos(np.pi * df[\"hour\"] / 12)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["full_merged_df=generate_features(full_merged_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["full_merged_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def cubic_root_transform(series):\n","    return np.cbrt(series)\n","\n","def cubic_root_transform_column(series):\n","    return np.cbrt(series)\n","\n","def cubic_root_transform_df(df, cols):\n","    for col in cols:\n","        if df[col].skew() >= 0.1:\n","            print('Right-skewed col:', col)\n","            df[col] = cubic_root_transform_column(df[col])\n","        elif df[col].skew() <= -0.1:\n","            df[col] = np.clip(df[col], df[col].quantile(0.01), df[col].quantile(0.99))\n","            print('Left-skewed col:', col)\n","            df[col] = cubic_root_transform_column(df[col])\n","    return df\n","\n","dropcols = ['county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'prediction_unit_id', 'year', 'month', 'day', 'hour','sin(dayofyear)','cos(dayofyear)','sin(hour)','cos(hour)']\n","transform_cols = list(full_merged_df.columns)\n","for cols in dropcols:\n","    transform_cols.remove(cols)\n","\n","full_merged_df = cubic_root_transform_df(full_merged_df, transform_cols)\n","\n","# apply standard scalar:\n","transform_cols.remove('target')\n","columns_to_scale=transform_cols\n","feature_scaler = StandardScaler()\n","# target_scaler = StandardScaler()\n","full_merged_df[columns_to_scale] = feature_scaler.fit_transform(full_merged_df[columns_to_scale])\n","# full_merged_df['target'] = target_scaler.fit_transform(full_merged_df[['target']])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["temp_df=full_merged_df[['county', 'is_business', 'product_type', 'target', 'is_consumption',\n","       'datetime', 'prediction_unit_id', 'year', 'month', 'day', 'hour',\n","       'lowest_price_per_mwh', 'highest_price_per_mwh', 'eic_count',\n","       'installed_capacity', 'euros_per_mwh']]\n","correlation_matrix = temp_df.corr()\n","import seaborn as sns\n","plt.figure(figsize=(8, 6))  # Specify the figure size\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Heatmap')  # Add a title\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["temp_df=full_merged_df[['county', 'is_business', 'product_type', 'target', 'is_consumption',\n","       'datetime', 'prediction_unit_id', 'year', 'month', 'day', 'hour',\n","       'lowest_price_per_mwh', 'highest_price_per_mwh', 'eic_count',\n","       'installed_capacity', 'euros_per_mwh']]\n","exclude_cols=['is_business', 'product_type','is_consumption','eic_count','installed_capacity','target','datetime']\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=5)  # Reduce to 2 dimensions\n","y_df=temp_df[exclude_cols]\n","df_pca = pca.fit_transform(temp_df.drop(columns=exclude_cols))\n","df_pca=pd.DataFrame(df_pca)\n","df_pca[exclude_cols]=y_df[exclude_cols]\n","correlation_matrix = df_pca.corr()\n","plt.figure(figsize=(8, 6))  # Specify the figure size\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Heatmap')  # Add a title\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["full_merged_df.drop(columns=['datetime'],inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.ensemble import VotingRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from catboost import CatBoostRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","X = full_merged_df.drop(columns=['target'])\n","y = full_merged_df['target']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize the regressors\n","xgb_regressor = XGBRegressor(n_estimators=600,\n","    learning_rate=0.02,\n","    max_depth= 5,\n","    subsample= 0.8,\n","    colsample_bytree=0.9)\n","lgbm_regressor = LGBMRegressor(\n","    n_estimators= 500,\n","    learning_rate= 0.1,\n","    max_depth= 6,\n","    subsample=0.9,\n","    colsample_bytree= 0.8)\n","catboost_regressor = CatBoostRegressor(\n","    iterations= 300,\n","    learning_rate= 0.1,\n","    depth= 5,\n","    subsample=0.9)\n","\n","# Create the ensemble\n","ensemble_regressor = VotingRegressor(estimators=[\n","    ('xgb', xgb_regressor),\n","    ('lgbm', lgbm_regressor),\n","    ('catboost', catboost_regressor)\n","])\n","\n","# Fit the ensemble on the training data\n","ensemble_regressor.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","ensemble_preds = ensemble_regressor.predict(X_test)\n","\n","# Evaluate the performance\n","rmse = mean_squared_error(y_test, ensemble_preds, squared=False)\n","print(f'Ensemble RMSE: {rmse}')\n","# Calculate the R^2 score\n","r2 = r2_score(y_test, ensemble_preds)\n","print(f'Ensemble R^2 Score: {r2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from sklearn.model_selection import train_test_split\n","# X_train, X_test, y_train, y_test = train_test_split(\n","#     full_merged_df.drop(columns='target'),\n","#     full_merged_df['target'],\n","#     test_size=0.33,\n","#     random_state=42\n","# )\n","# p_1={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.004811751415536496, 'colsample_bytree': 0.8841841689852410, 'colsample_bynode': 0.4305836942635745, 'reg_alpha': 3.11984157361821, 'reg_lambda': 1.088469732297296, 'min_data_in_leaf': 162, 'max_depth': 16,'device':'gpu' , 'num_leaves': 435,'n_jobs':4}\n","# p0 ={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.007922862526647507, 'colsample_bytree': 0.9052952790963521, 'colsample_bynode': 0.4416947152746856, 'reg_alpha': 3.31471952672932, 'reg_lambda': 1.349570843308307, 'min_data_in_leaf': 185, 'max_depth': 18,'device':'gpu' , 'num_leaves': 445,'n_jobs':4}\n","# p1 ={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.010339736147758608, 'colsample_bytree': 0.9263063801074632, 'colsample_bynode': 0.4527058263857967, 'reg_alpha': 3.62802063709343, 'reg_lambda': 1.650681954419419, 'min_data_in_leaf': 201, 'max_depth': 20,'device':'gpu' , 'num_leaves': 455,'n_jobs':4}\n","# p2 ={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.013090718804096083, 'colsample_bytree': 0.9499770953943448, 'colsample_bynode': 0.4670163857441046, 'reg_aplha': 3.96946065556807, 'reg_lambda': 1.925712107567988, 'min_data_in_leaf': 223, 'max_depth': 22,'device':'gpu' , 'num_leaves': 465,'n_jobs':4}\n","# p3 ={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.015559490612977255, 'colsample_bytree': 0.9682791614810814, 'colsample_bynode': 0.4722023075509447, 'reg_aplha': 4.15624585398345, 'reg_lambda': 2.265053303366992, 'min_data_in_leaf': 254, 'max_depth': 24,'device':'gpu' , 'num_leaves': 475,'n_jobs':4}\n","# p4 ={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.018908744594789185, 'colsample_bytree': 0.9864875442500248, 'colsample_bynode': 0.4832525869590394, 'reg_aplha': 4.35845913192557, 'reg_lambda': 2.355521088983217, 'min_data_in_leaf': 289, 'max_depth': 26,'device':'gpu' , 'num_leaves': 485,'n_jobs':4}\n","# p5 ={'n_iter':4000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.021819855605890296, 'colsample_bytree': 0.9995986553611359, 'colsample_bynode': 0.4953634970601405, 'reg_alpha': 4.58956024201648, 'reg_lambda': 2.616432197094328, 'min_data_in_leaf': 309, 'max_depth': 28,'device':'gpu' , 'num_leaves': 495,'n_jobs':4}\n","# lgbp_1=LGBMRegressor(**p_1)\n","# lgbp0=LGBMRegressor(**p0)\n","# lgbp1=LGBMRegressor(**p1)\n","# lgbp2=LGBMRegressor(**p2)\n","# lgbp3=LGBMRegressor(**p3)\n","# lgbp4=LGBMRegressor(**p4)\n","# lgbp5=LGBMRegressor(**p5)\n","\n","# for lgbm_model in [lgbp_1, lgbp0, lgbp1, lgbp2, lgbp3, lgbp4, lgbp5]: # v1\n","# # for lgbm_model in [lgbp_1, lgbp1, lgbp2, lgbp3, lgbp4]:\n","#     print('_______________________________________________________')\n","#     print('Start')\n","#     lgbm_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], callbacks=[\n","#             lgb.callback.early_stopping(stopping_rounds=100),\n","#             lgb.callback.log_evaluation(period=100),\n","#         ],)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from sklearn.preprocessing import normalize\n","# import enefit\n","# import pandas as pd\n","# import numpy as np\n","# env = enefit.make_env()\n","# iter_test = env.iter_test()\n","# for (train, t_re, client, hist,\n","#     fore, elec, gas, sample_prediction) in iter_test:\n","#     gas = gas.tail(1)\n","#     train['data_id']=0\n","#     client['data_id']=0\n","#     hist['data_id']=0\n","#     fore['data_id']=0\n","#     elec['data_id']=0\n","#     gas['data_id']=0  \n","    \n","#     train['prediction_datetime']=pd.to_datetime(train['prediction_datetime'])\n","\n","    \n","    \n","#     train.rename(columns={'prediction_datetime':'datetime'},inplace=True)\n","    \n","#     elec['forecast_date']=pd.to_datetime(elec['forecast_date'])\n","#     elec.rename(columns={'forecast_date':'datetime'},inplace=True)\n","\n","    \n","    \n","#     fore['forecast_datetime']=pd.to_datetime(fore['forecast_datetime'])\n","#     fore.rename(columns={'forecast_datetime':'datetime'},inplace=True)\n","\n","\n","\n","#     fore['latitude']=fore['latitude'].astype('float').round(1)\n","#     fore['longitude']=fore['longitude'].astype('float').round(1)\n","#     fore_location=pd.merge(fore,location,on=['latitude','longitude'],how='left').dropna(axis=0).drop(columns=['hours_ahead','origin_datetime','latitude','longitude','data_id'])\n","#     fore_location['county']=fore_location['county'].astype('int')\n","    \n","    \n","    \n","#     for cols in fore_location.columns:      \n","#         if(cols!='county' and cols!='datetime' and cols!='data_id'):\n","#             new_name=f'forecast_{cols}'\n","#             fore_location.rename(columns={cols:new_name},inplace=True)    \n","    \n","#     fore_location['datetime']=pd.to_datetime(fore_location['datetime'])\n","    \n","#     fore_location_overall=fore_location.groupby(by=['datetime']).mean().drop(columns=['county']).reset_index()\n","\n","#     fore_location_overall['datetime']=pd.to_datetime(fore_location_overall['datetime'])\n","\n","\n","    \n","#     fore_location_county=fore_location.groupby(by=['datetime','county']).mean().reset_index()\n","\n","#     fore_location_county['datetime']=pd.to_datetime(fore_location_overall['datetime'])\n","    \n","    \n","#     for cols in fore_location_overall.columns:\n","#         if(cols!='datetime' and cols!='data_id' and cols!='county'):\n","#             new_name=f'overall_{cols}'\n","#             fore_location_overall.rename(columns={cols:new_name},inplace=True)\n","#     for cols in fore_location_county.columns:\n","#         if(cols!='datetime' and cols!='county' and cols!='data_id'):\n","#             new_name=f'local_{cols}'\n","#             fore_location_county.rename(columns={cols:new_name},inplace=True)\n","\n","\n","            \n","#     hist['latitude']=hist['latitude'].astype('float').round(1)\n","#     hist['longitude']=hist['longitude'].astype('float').round(1)\n","#     location_df_history=pd.merge(hist,location,on=['latitude','longitude'],how='left').dropna().drop(columns=['latitude','longitude'])\n","\n","#     for cols in location_df_history.columns:\n","#         if(cols!='county' and cols!='datetime' and cols!='data_id'):\n","#             new_name=f'historic_{cols}'\n","#             location_df_history.rename(columns={cols:new_name},inplace=True)\n","#     location_df_history['county']=location_df_history['county'].astype('int')\n","#     location_df_history['datetime']=pd.to_datetime(location_df_history['datetime'])\n","    \n","#     df_overall_historic=location_df_history.groupby(by=['datetime']).mean().reset_index().drop(columns=['county','data_id'])\n","#     df_overall_historic['datetime']=pd.to_datetime(df_overall_historic['datetime'])\n","#     df_overall_historic=pd.merge(df_overall_historic,location_df_history[['datetime', 'data_id']],on=['datetime'],how='left')\n","    \n","#     df_local_historic=location_df_history.groupby(by=['datetime','county']).mean().reset_index().drop(columns=['data_id'])\n","#     df_local_historic['datetime']=pd.to_datetime(df_local_historic['datetime'])\n","#     df_local_historic=pd.merge(df_local_historic,location_df_history[['datetime', 'data_id']],on=['datetime'],how='left')\n","    \n","#     for cols in df_overall_historic.columns:\n","#         if(cols!='datetime' and cols!='data_id' and cols!='county'):\n","#             new_name=f'overall_{cols}'\n","#             df_overall_historic.rename(columns={cols:new_name},inplace=True)\n","#     for cols in df_local_historic.columns:\n","#         if(cols!='datetime' and cols!='county' and cols!='data_id'):\n","#             new_name=f'local_{cols}'\n","#             df_local_historic.rename(columns={cols:new_name},inplace=True)\n","            \n","            \n","#     train['year']=train['datetime'].dt.year\n","#     train['month']=train['datetime'].dt.month\n","#     train['day']=train['datetime'].dt.day\n","#     train['hour']=train['datetime'].dt.hour            \n","    \n","    \n","#     elec['hour'] = elec['datetime'].dt.hour\n","    \n","    \n","    \n","# #     merged_data= pd.merge(train,client.drop(columns = ['date']), how='left', on=['data_id', 'county', 'is_business', 'product_type'])\n","#     merged_data= pd.merge(train,gas[['data_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_id')\n","#     merged_data= pd.merge(merged_data,client.drop(columns = ['date']), how='left', on=['data_id', 'county', 'is_business', 'product_type'])\n","#     merged_data= pd.merge(merged_data,elec[['euros_per_mwh', 'hour', 'data_id']], how='left', on=['hour', 'data_id'])\n","#     merged_data= pd.merge(merged_data,fore_location_overall, how='left', on=['datetime'])\n","#     merged_data= pd.merge(merged_data,fore_location_county, how='left', on=['datetime', 'county'])\n","    \n","    \n","    \n","    \n","#     df_overall_historic['hour']= df_overall_historic['datetime'].dt.hour\n","#     df_local_historic['hour']= df_local_historic['datetime'].dt.hour\n","\n","#     df_overall_historic.drop_duplicates(inplace=True)\n","#     df_local_historic.drop_duplicates(inplace=True)\n","#     df_overall_historic.drop('datetime', axis= 1, inplace= True)\n","#     df_local_historic.drop('datetime', axis= 1, inplace= True)\n","\n","    \n","#     merged_data= merged_data.merge(df_overall_historic, how='left', on=['data_id', 'hour'])\n","\n","#     merged_data= merged_data.merge(df_local_historic, how='left', on=['data_id', 'county', 'hour'])\n","   \n","#     merged_data=merged_data.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n","\n","#     merged_data.drop(['level_0', 'level_1', 'row_id', 'data_id','currently_scored'], axis= 1, inplace= True)\n","#     merged_data=generate_features(merged_data)\n","#     merged_data.drop(columns='datetime')\n","#     test_merge = cubic_root_transform_df(merged_data, transform_cols)\n","#     test_merge[columns_to_scale] = feature_scaler.transform(test_merge[columns_to_scale])\n","    \n","#     y_preds=ensemble_regressor.predict(test_merge)\n","# #     inverse_scaled_values = target_scaler.inverse_transform([y_preds])\n","# #     gc.collect()\n","    \n","# #     sample_prediction[\"target\"] = inverse_scaled_values.reshape(-1,1)\n","#     sample_prediction[\"target\"]=y_preds**3\n","#     env.predict(sample_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7292407,"sourceId":57236,"sourceType":"competition"}],"dockerImageVersionId":30558,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
